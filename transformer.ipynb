{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cf5d41-d2a8-47a8-ab74-c1bc5b546062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import (\n",
    "    Module,\n",
    "    ModuleList,\n",
    "    Sequential,\n",
    "    Parameter,\n",
    "    Linear, \n",
    "    Dropout,\n",
    "    LayerNorm,\n",
    "    Softmax,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from einops import rearrange as re\n",
    "from opt_einsum import contract as einsum\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a0df6b-3e48-413a-8b22-30039a880740",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c61cb0-2b8d-4d2d-88a6-9ef6bec3ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217668ed-d993-4f1f-808a-7b1b1e467b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat(x):\n",
    "    df = pd.DataFrame(x.detach().numpy())\n",
    "    return df.style.background_gradient(cmap='Blues')  # .format('{:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39030dd9-81af-4c70-8567-1994e592bf35",
   "metadata": {},
   "source": [
    "[Attention is All You Need](https://arxiv.org/abs/1706.03762?context=cs)  \n",
    "[Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238)  \n",
    "[Transformer Language Model Mathematical Definition](https://www.apronus.com/math/transformer-language-model-definition)  \n",
    "[AI Explained - 3D viz of transformer structure](https://youtu.be/-9vVhYEXeyQ?t=456)  \n",
    "[Bloem Transformer Implementation source](https://github.com/pbloem/former/blob/master/former/modules.py)  \n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks)  \n",
    "[Pytorch multi_head_attention_forward source](https://github.com/pytorch/pytorch/blob/dcf51885618e7d1d9aa6e628f3354f67ad82b446/torch/nn/functional.py#L4917)   \n",
    "[Writing a better code with pytorch and einops](http://einops.rocks/pytorch-examples.html)  \n",
    "[R-Drop: Regularized Dropout for Neural Networks](https://arxiv.org/pdf/2106.14448v2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffafc46e-0b8f-41b3-be52-c0856ec506b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model=512, n_blocks=6, vocab=30_000):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model)\n",
    "        self.encoder = Stack(EncoderBlock, n_blocks)\n",
    "        self.decoder = Stack(DecoderBlock, n_blocks)\n",
    "        self.head = Sequential(Linear(d_model, vocab), Softmax(dim=-1))\n",
    "       \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        ctx = self.encoder(src, mask=src_mask)\n",
    "        return self.head(self.decoder(tgt, ctx=ctx, mask=tgt_mask, ctx_mask=tgt_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4874cf2f-fc73-4dfe-804c-c36c15789d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack(nn.Sequential):\n",
    "    def __init__(self, Layer, n):\n",
    "        stack = [copy.deepcopy(Layer()) for l in range(n)]\n",
    "        super().__init__(*stack)\n",
    "        \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        for module in self:\n",
    "            x = module(x, *args, **kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08d23126-22ce-4424-b818-26bab1b2a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention()\n",
    "        self.ff = FeedForward()\n",
    "        \n",
    "    def forward(self, x, *, mask=None):\n",
    "        return self.ff(self.mha(x, mask=mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77fac8b5-9f09-4f55-a830-01ec26064552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.masked_mha = MultiHeadAttention()\n",
    "        self.mha = MultiHeadAttention()\n",
    "        self.ff = FeedForward()\n",
    "        \n",
    "    def forward(self, x, *, ctx=None, mask=None, ctx_mask=None):\n",
    "        x = self.masked_mha(x, mask=mask)\n",
    "        return self.ff(self.mha(x, ctx, ctx, ctx_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac798568-9abf-4811-9529-6f68b5cebae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(x + self._ff(x))\n",
    "    \n",
    "    def _ff(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5594a1c-9d36-4a90-abc1-4f830c893704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int = 8, emb_dim: int = 512):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0  # d_h = d_q = d_k = d_v = emb_dim // n_heads\n",
    "        self.proj_qkv = [Linear(emb_dim, emb_dim, bias=False) for _ in range(3)]\n",
    "        self.proj_o = Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.h, self.d = n_heads, emb_dim\n",
    "\n",
    "    def forward(self, q, k=None, v=None, mask=None):\n",
    "        \"\"\" q, k, v: (batch, seq_len, emb_dim) mask: (seq_len, seq_len) \"\"\"\n",
    "        if k is None and v is None:\n",
    "            k = v = q\n",
    "        if mask is None:\n",
    "            mask = torch.zeros((q.shape[-2]))\n",
    "        return self.norm(q + self._mha(q, k, v, mask))\n",
    "    \n",
    "    def _mha(self, q, k, v, mask):\n",
    "        q, k, v = (proj(x) for x, proj in zip((q, k, v), self.proj_qkv))\n",
    "        q, k, v = (re(x, \"b l (h d) -> b h l d\", h=self.h) for x in (q, k, v))\n",
    "        attn = einsum(\"...ij,...kj->...ik\", q, k)\n",
    "        attn = mask + torch.einsum(\"...ij,...kj->...ik\", q, k)\n",
    "        attn = F.softmax(attn / q.shape[-1] ** (1/2), dim=-1)\n",
    "        out = einsum(\"...ij,...jk->...ik\", attn, v)\n",
    "        out = re(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.proj_o(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6132f4e3-a172-46d5-9e1a-f490b0c578c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHAInference(MultiHeadAttention):\n",
    "    def __init__(self, n_heads: int = 8, emb_dim: int = 512):\n",
    "        super().__init__(n_heads, emb_dim)\n",
    "        self.attn = torch.empty((1000 * 8, 256, 256))\n",
    "        self.out = torch.empty((1000 * 8, 256, 64))\n",
    "\n",
    "    def _mha(self, q, k, v, mask):\n",
    "        with torch.no_grad():\n",
    "            q, k, v = (proj(x) for x, proj in zip((q, k, v), self.proj_qkv))\n",
    "            q, k, v = (re(x, \"b l (h d_h) -> (b h) l d_h\", h=self.h) for x in (q, k, v))\n",
    "            torch.bmm(q, re(k, \"bh l d_h -> bh d_h l\"), out=self.attn)\n",
    "            self.attn += mask\n",
    "            self.attn /= q.shape[-1] ** (1/2)\n",
    "            self.attn = F.softmax(self.attn, dim=-1)\n",
    "            torch.bmm(self.attn, v, out=self.out)\n",
    "            return self.proj_o(re(self.out, \"(b h) l d_h -> b l (h d_h)\", h=self.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15b557-e0de-4492-8d92-ee6a1980b05f",
   "metadata": {},
   "source": [
    "# Layer Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39a8c7db-7f3b-4f6d-a9b8-55dd289adc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, l, d = 1, 256, 512\n",
    "sh = (b, l, d)\n",
    "v = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0907fdae-0c1c-4fa8-b998-0395b4a26f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.rand(sh)\n",
    "mask = torch.triu(torch.full((l, l), -torch.inf), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acc8214e-63db-436b-8050-291bb442abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54 ms, sys: 23.5 ms, total: 77.5 ms\n",
      "Wall time: 21.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time assert MultiHeadAttention()(emb).shape == sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e889d135-a7fc-4246-9432-201c78e673ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.8 ms, sys: 7.96 ms, total: 75.8 ms\n",
      "Wall time: 20.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time assert FeedForward()(emb).shape == sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90d5c7c1-4095-4f84-aabc-29b67d1db6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 30.3 ms, total: 154 ms\n",
      "Wall time: 36.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time assert EncoderBlock()(emb).shape == sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c21fe8f9-9356-4851-a442-f9e8d7989755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 ms, sys: 32.4 ms, total: 204 ms\n",
      "Wall time: 51.6 ms\n"
     ]
    }
   ],
   "source": [
    "%time assert DecoderBlock()(emb).shape == sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2c022c3-d840-427b-91ed-b68881cb1601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.5 ms, sys: 9.98 ms, total: 68.5 ms\n",
      "Wall time: 14.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time assert MultiHeadAttention()(emb).shape == sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45502bc0-6551-44ee-85c5-3c2f860c6c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.49 s, sys: 229 ms, total: 3.72 s\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%time assert Transformer()(emb, emb, mask, mask).shape == (b, l, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23d64f-7625-4e9e-80cc-5d775774eaef",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b1fe18-6173-43cb-aa3e-e50b069287f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.rand((1_000, 256, 512))\n",
    "mask = torch.triu(torch.full((256, 256), -torch.inf), diagonal=1)\n",
    "mha = MultiHeadAttention()\n",
    "mha.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f18c0-0c62-405d-ae78-6729e4107667",
   "metadata": {},
   "source": [
    "#### _mha vs _mha_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad3e64c-aeee-44a8-942f-a06e9e1ca918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.83 s ± 69.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = mha._mha(emb, emb, emb, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284e0116-0df5-4d59-a314-dc5bd68cd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_infer = MHAInference()\n",
    "mha_infer.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca96857-4056-4c8f-a143-e9fc68221a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.32 s ± 27.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = mha_infer(emb, emb, emb, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a32035f1-0a9c-4802-96d9-55dd8e113dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 0.001 s\n",
       "\n",
       "Total time: 2.88578 s\n",
       "File: /var/folders/5y/b092b3m96yb8nglxy9dzqbnr0000gn/T/ipykernel_51150/1451311884.py\n",
       "Function: _mha at line 68\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    68                                               def _mha(self, q, k, v, mask):\n",
       "    69         1        633.7    633.7     22.0          q, k, v = (proj(x) for x, proj in zip((q, k, v), self.proj_qkv))\n",
       "    70         1          0.1      0.1      0.0          q, k, v = (re(x, \"b l (h d) -> b h l d\", h=self.h) for x in (q, k, v))\n",
       "    71         1        355.6    355.6     12.3          attn = einsum(\"...ij,...kj->...ik\", q, k)\n",
       "    72         1        840.5    840.5     29.1          attn = mask + torch.einsum(\"...ij,...kj->...ik\", q, k)\n",
       "    73         1        633.2    633.2     21.9          attn = F.softmax(attn / q.shape[-1] ** (1/2), dim=-1)\n",
       "    74         1        200.7    200.7      7.0          out = einsum(\"...ij,...jk->...ik\", attn, v)\n",
       "    75         1          0.1      0.1      0.0          out = re(out, \"b h n d -> b n (h d)\")\n",
       "    76         1        221.9    221.9      7.7          out = self.proj_o(out)\n",
       "    77         1          0.0      0.0      0.0          return out"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -u 0.001 -f mha._mha mha(emb, emb, emb, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ee3cce2-44d2-4ca7-b677-a11b9c293f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 0.001 s\n",
       "\n",
       "Total time: 0 s\n",
       "File: /var/folders/5y/b092b3m96yb8nglxy9dzqbnr0000gn/T/ipykernel_51150/1451311884.py\n",
       "Function: _mha at line 68\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    68                                               def _mha(self, q, k, v, mask):\n",
       "    69                                                   q, k, v = (proj(x) for x, proj in zip((q, k, v), self.proj_qkv))\n",
       "    70                                                   q, k, v = (re(x, \"b l (h d) -> b h l d\", h=self.h) for x in (q, k, v))\n",
       "    71                                                   attn = einsum(\"...ij,...kj->...ik\", q, k)\n",
       "    72                                                   attn = mask + torch.einsum(\"...ij,...kj->...ik\", q, k)\n",
       "    73                                                   attn = F.softmax(attn / q.shape[-1] ** (1/2), dim=-1)\n",
       "    74                                                   out = einsum(\"...ij,...jk->...ik\", attn, v)\n",
       "    75                                                   out = re(out, \"b h n d -> b n (h d)\")\n",
       "    76                                                   out = self.proj_o(out)\n",
       "    77                                                   return out"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -u 0.001 -f mha_infer._mha mha_infer(emb, emb, emb, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f45895-2166-41f1-84b1-d4efd7dbc988",
   "metadata": {},
   "source": [
    "#### Pytorch [MultiheadAttention.forward](https://github.com/pytorch/pytorch/blob/bbe8d019f280478dc3b143f6988e3e5668499f28/torch/nn/modules/activation.py#L1010) local\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4fbd57a-31b7-4922-984a-ea7c6bad2a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_emb = re(emb, 'b l d -> l b d')\n",
    "bsz, tgt_len, embed_dim = emb.shape\n",
    "num_heads, head_dim = 8, 64\n",
    "in_proj_weight = nn.Parameter(torch.vstack([l.weight for l in mha.proj_qkv]))\n",
    "out_proj_weight = mha.proj_o.weight\n",
    "th_proj_o = Linear(512, 512, bias=False)\n",
    "th_proj_o.weight = out_proj_weight\n",
    "\n",
    "def th_mha():\n",
    "    th_q, th_k, th_v = F._in_projection_packed(th_emb, th_emb, th_emb, in_proj_weight, None)\n",
    "    th_q = th_q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    th_k = th_k.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    th_v = th_v.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    th_out, th_attn = F._scaled_dot_product_attention(th_q, th_k, th_v, mask, 0.0)\n",
    "    th_out = th_out.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
    "    th_out = th_proj_o(th_out)\n",
    "    th_out = th_out.view(tgt_len, bsz, th_out.size(1))\n",
    "    th_out = mha.norm(th_emb + th_out)\n",
    "    return re(th_out, 'l b d -> b l d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bbfab-22f4-414f-95d4-7a468bb2a046",
   "metadata": {},
   "source": [
    "#### vs. Pytorch F.multi_head_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71e20efa-d1a7-47ca-9eee-d72a98bc1207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13 s ± 92.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mha_out = mha(emb, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba9bc7d4-0c92-43d6-b5bc-e78ee503c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.67 s ± 43.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "th_out = th_mha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b08b0f47-adff-4cbb-8b71-c3eb35d68d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 0.001 s\n",
       "\n",
       "Total time: 2.59382 s\n",
       "File: /var/folders/5y/b092b3m96yb8nglxy9dzqbnr0000gn/T/ipykernel_51150/486783080.py\n",
       "Function: th_mha at line 9\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     9                                           def th_mha():\n",
       "    10         1        821.6    821.6     31.7      th_q, th_k, th_v = F._in_projection_packed(th_emb, th_emb, th_emb, in_proj_weight, None)\n",
       "    11         1         52.8     52.8      2.0      th_q = th_q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
       "    12         1         55.9     55.9      2.2      th_k = th_k.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
       "    13         1        126.6    126.6      4.9      th_v = th_v.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
       "    14         1        989.2    989.2     38.1      th_out, th_attn = F._scaled_dot_product_attention(th_q, th_k, th_v, mask, 0.0)\n",
       "    15         1         87.4     87.4      3.4      th_out = th_out.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
       "    16         1        223.0    223.0      8.6      th_out = th_proj_o(th_out)\n",
       "    17         1          0.0      0.0      0.0      th_out = th_out.view(tgt_len, bsz, th_out.size(1))\n",
       "    18         1        237.2    237.2      9.1      th_out = mha.norm(th_emb + th_out)\n",
       "    19         1          0.1      0.1      0.0      return re(th_out, 'l b d -> b l d')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -u 0.001 -f th_mha th_out = th_mha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fa59170-ae37-4b41-a410-7887ccebb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_out = mha(emb, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "468aaf29-8fb0-468b-94df-5e10aea829c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(mha_out, th_out, atol=1e-6, equal_nan=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
